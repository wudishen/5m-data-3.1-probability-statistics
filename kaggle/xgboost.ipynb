{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a47831ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set shape: (1812, 51)\n",
      "Predict set shape: (453, 45)\n",
      "Average wins: 79.26158940397352\n",
      "Using 10 features: ['R', 'H', 'SV', 'ERA', 'CG', 'SHO', 'IPouts', 'RA', 'pyth_wins', 'run_diff_per_game']\n",
      "   decade_1910  decade_1920  decade_1930\n",
      "0        False        False         True\n",
      "1        False        False        False\n",
      "2        False        False        False\n",
      "3        False        False         True\n",
      "4        False        False        False\n",
      "5        False        False         True\n",
      "6         True        False        False\n",
      "7        False        False        False\n",
      "8        False        False        False\n",
      "9        False        False        False\n",
      "Fold 1:\n",
      "  Test MAE = 2.9109\n",
      "  Best iteration = 6802\n",
      "Fold 2:\n",
      "  Test MAE = 2.9162\n",
      "  Best iteration = 8040\n",
      "Fold 3:\n",
      "  Test MAE = 3.0464\n",
      "  Best iteration = 2759\n",
      "Fold 4:\n",
      "  Test MAE = 2.9374\n",
      "  Best iteration = 5487\n",
      "Fold 5:\n",
      "  Test MAE = 3.0131\n",
      "  Best iteration = 4305\n",
      "\n",
      "GroupKFold CV MAE scores: [2.9109459866534224, 2.916227830854874, 3.0464319410270817, 2.9374378913087273, 3.013070903840612]\n",
      "Mean CV MAE: 2.9648\n",
      "Std CV MAE:  0.0548\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import xgboost as xgb\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 1. Feature engineering function (your version)\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "def add_strong_features(df):\n",
    "    df['run_diff'] = df['R'] - df['RA']\n",
    "    df['run_diff_per_game'] = df['run_diff'] / df['G']\n",
    "    \n",
    "    exp = 1.86\n",
    "    df['pyth_exp'] = np.where(\n",
    "        df['R'] + df['RA'] > 0,\n",
    "        df['R']**exp / (df['R']**exp + df['RA']**exp),\n",
    "        0.5\n",
    "    )\n",
    "    df['pyth_wins'] = np.round(df['G'] * df['pyth_exp']).astype(int)\n",
    "    \n",
    "    df['HR_diff'] = df['HR'] - df['HRA']\n",
    "    df['ERA_adj'] = df['ERA'] - df['mlb_rpg'] * 9\n",
    "\n",
    "    df['pitch_dom_low'] = df['ERA'] / (df['mlb_rpg'] + 1)\n",
    "\n",
    "     # Per-game rates (helps across eras)\n",
    "    df['R_per_game']  = df['R'] / df['G']\n",
    "    df['RA_per_game'] = df['RA'] / df['G']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 2. Load data\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "BASE = \"input/\"\n",
    "data_df = pd.read_csv(os.path.join(BASE, \"data.csv\"))\n",
    "predict_df = pd.read_csv(os.path.join(BASE, \"predict.csv\"))\n",
    "\n",
    "print(f\"Data set shape: {data_df.shape}\")\n",
    "print(f\"Predict set shape: {predict_df.shape}\")\n",
    "print(\"Average wins:\", data_df['W'].mean())\n",
    "\n",
    "# Apply feature engineering to BOTH datasets\n",
    "data_df   = add_strong_features(data_df)\n",
    "predict_df = add_strong_features(predict_df)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 3. Define features (your current list)\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "features = [\n",
    "    'R', 'H', 'SV', 'ERA', 'CG', 'SHO',\n",
    "    'IPouts','RA',\n",
    "     'pyth_wins', 'run_diff_per_game',\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "features = [f for f in features if f in data_df.columns and f in predict_df.columns]\n",
    "print(f\"Using {len(features)} features:\", features)\n",
    "\n",
    "X = data_df[features]\n",
    "y = data_df['W']\n",
    "\n",
    "print(data_df[['decade_1910', 'decade_1920', 'decade_1930']].head(10))\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 4. GroupKFold by yearID (realistic validation)\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "groups = data_df['yearID']\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "fold_maes = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(gkf.split(X, y, groups=groups), 1):\n",
    "    print(f\"Fold {fold}:\")\n",
    "    \n",
    "    X_train_fold = X.iloc[train_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    X_test_fold  = X.iloc[test_idx]\n",
    "    y_test_fold  = y.iloc[test_idx]\n",
    "    \n",
    "    # XGBoost does NOT need scaling — use raw features\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators           = 12000,\n",
    "        learning_rate          = 0.002,\n",
    "        max_depth              = 4,\n",
    "        min_child_weight       = 10,\n",
    "        subsample              = 0.8,\n",
    "        colsample_bytree       = 0.7,\n",
    "        reg_lambda             = 15.0,\n",
    "        reg_alpha              = 4.0,\n",
    "        gamma                  = 0.1,\n",
    "        random_state           = 42,\n",
    "        tree_method            = 'hist',\n",
    "        eval_metric            = 'mae',\n",
    "        early_stopping_rounds  = 200\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=[(X_test_fold, y_test_fold)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    test_preds = model.predict(X_test_fold)\n",
    "    fold_mae = mean_absolute_error(y_test_fold, test_preds)\n",
    "    fold_maes.append(fold_mae)\n",
    "    \n",
    "    print(f\"  Test MAE = {fold_mae:.4f}\")\n",
    "    print(f\"  Best iteration = {model.best_iteration}\")\n",
    "\n",
    "print(\"\\nGroupKFold CV MAE scores:\", fold_maes)\n",
    "print(f\"Mean CV MAE: {np.mean(fold_maes):.4f}\")\n",
    "print(f\"Std CV MAE:  {np.std(fold_maes):.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a062e4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set shape: (1812, 51)\n",
      "Predict set shape: (453, 45)\n",
      "Number of available default features: 24\n",
      "['G', 'R', 'AB', 'H', 'HR', 'SO', 'SB', 'SV', 'ERA', 'CG', 'SHO', 'IPouts', 'HA', 'HRA', 'BBA', 'SOA', 'E', 'DP', 'FP', 'mlb_rpg', 'pyth_wins', 'run_diff_per_game', 'era_7', 'era_8']\n",
      "Using 24 features: ['G', 'R', 'AB', 'H', 'HR', 'SO', 'SB', 'SV', 'ERA', 'CG', 'SHO', 'IPouts', 'HA', 'HRA', 'BBA', 'SOA', 'E', 'DP', 'FP', 'mlb_rpg', 'pyth_wins', 'run_diff_per_game', 'era_7', 'era_8']\n",
      "Fold 1:\n",
      "Fold 2:\n",
      "Fold 3:\n",
      "Fold 4:\n",
      "Fold 5:\n",
      " Test MAE = 2.8794\n",
      " Test Line MAE = 2.8745\n",
      " Test Ridge MAE = 2.8741\n",
      " Test XGB MAE = 3.0313\n",
      "\n",
      "GroupKFold results by fold:\n",
      "   fold  test_mae  test_line_mae  test_ridge_mae  test_xgb_mae\n",
      "0     1  2.728905       2.729393        2.732792      2.936844\n",
      "1     2  2.725462       2.735606        2.735811      2.912866\n",
      "2     3  2.853815       2.880186        2.879473      3.076895\n",
      "3     4  2.661901       2.664642        2.666928      2.961760\n",
      "4     5  2.879402       2.874454        2.874127      3.031308\n",
      "\n",
      "Average across 5 folds:\n",
      "fold              3.000000\n",
      "test_mae          2.769897\n",
      "test_line_mae     2.776856\n",
      "test_ridge_mae    2.777826\n",
      "test_xgb_mae      2.983935\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "def add_strong_features(df):\n",
    "    # Core run-related (usually the strongest predictors)\n",
    "    \n",
    "    df['run_diff'] = df['R'] - df['RA']\n",
    "    \n",
    "    df['run_diff_per_game'] = df['run_diff'] / df['G']\n",
    "    \n",
    "    # Pythagorean (modern exponent)\n",
    "    exp = 1.86\n",
    "    df['pyth_exp'] = np.where(\n",
    "        df['R'] + df['RA'] > 0,\n",
    "        df['R']**exp / (df['R']**exp + df['RA']**exp),\n",
    "        0.5\n",
    "    )\n",
    "    df['pyth_wins'] = np.round(df['G'] * df['pyth_exp']).astype(int)\n",
    "    \n",
    "   \n",
    "    \"\"\"\n",
    "    # Per-game rates (helps across eras)\n",
    "    df['R_per_game']  = df['R'] / df['G']\n",
    "    df['RA_per_game'] = df['RA'] / df['G']\n",
    "    \n",
    "    # Pitching quality\n",
    "    df['IP'] = df['IPouts'] / 3.0 + 1e-6          # avoid division by zero\n",
    "    df['SOA_per_game'] = df['SOA'] / df['IP']\n",
    "    df['WHIP'] = (df['HA'] + df['BBA']) / df['IP']\n",
    "    \"\"\"\n",
    "    # Other useful differentials & adjustments\n",
    "    df['HR_diff'] = df['HR'] - df['HRA']\n",
    "    df['ERA_adj'] = df['ERA'] - df['mlb_rpg'] * 9     # rough league adjustment\n",
    "    \n",
    "    # Late-game / bullpen strength\n",
    "    # df['SV_rate'] = df['SV'] / (df['SV'] + df['BS'] + 1e-6)  # if you have blown saves\n",
    "    \"\"\"\n",
    "    # Market/team quality proxy\n",
    "    #df['attendance_per_game'] = df['attendance'] / df['G']\n",
    "    \"\"\"\n",
    "    return df\n",
    "\n",
    "# Load the pre-processed train and test datasets\n",
    "#DATAPATH = \"sctpdsai-m-3-ds-3-coaching-money-ball-analytics\"\n",
    "BASE = f\"input/\"\n",
    "data_df = pd.read_csv(os.path.join(BASE, \"data.csv\"))\n",
    "predict_df = pd.read_csv(os.path.join(BASE, \"predict.csv\"))\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(f\"Data set shape: {data_df.shape}\")\n",
    "print(f\"Predict set shape: {predict_df.shape}\")\n",
    "\n",
    "\n",
    "# 3. Define features (unchanged)\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "features = [\n",
    "'G', 'R', 'AB', 'H', 'HR', 'SO', 'SB', 'SV', 'ERA', 'CG', 'SHO',\n",
    "'IPouts', 'HA', 'HRA', 'BBA', 'SOA', 'E', 'DP', 'FP',\n",
    "'mlb_rpg', 'pyth_wins', 'run_diff_per_game',\n",
    "'era_7', 'era_8'\n",
    "]\n",
    "\n",
    "data_df   = add_strong_features(data_df)\n",
    "predict_df = add_strong_features(predict_df)\n",
    "\n",
    "# Filter features that exist in both datasets\n",
    "available_features = [col for col in features if col in data_df.columns and col in predict_df.columns]\n",
    "print(f\"Number of available default features: {len(available_features)}\")\n",
    "print(available_features)\n",
    "\n",
    "features = [f for f in features if f in data_df.columns and f in predict_df.columns]\n",
    "print(f\"Using {len(features)} features:\", features)\n",
    "X = data_df[features]\n",
    "y = data_df['W']\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 4. GroupKFold by yearID (the key change!)\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "from sklearn.model_selection import GroupKFold\n",
    "# Use yearID as the group → no leakage across years\n",
    "groups = data_df['yearID']\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "fold_results = []\n",
    "for fold, (train_idx, test_idx) in enumerate(gkf.split(X, y, groups=groups), 1):\n",
    "    print(f\"Fold {fold}:\")\n",
    "\n",
    "    X_train_fold = X.iloc[train_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    X_test_fold = X.iloc[test_idx]\n",
    "    y_test_fold = y.iloc[test_idx]\n",
    "\n",
    "# Scale inside each fold (correct for linear models)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_fold)\n",
    "    X_test_scaled = scaler.transform(X_test_fold)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_scaled, y_train_fold)\n",
    "\n",
    "# Use RidgeCV (your current best local performer)\n",
    "    alphas = np.logspace(-4, 2, 30)\n",
    "    ridge = RidgeCV(alphas=alphas, cv=3, scoring='neg_mean_absolute_error') # inner CV=3 to speed up\n",
    "    ridge.fit(X_train_scaled, y_train_fold)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=[(X_test_fold, y_test_fold)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "# Predict\n",
    "    xgb_test_preds = model.predict(X_test_fold)\n",
    "   # train_preds = lr.predict(X_train_scaled)\n",
    "    test_preds = ridge.predict(X_test_scaled)\n",
    "    line_test_preds = lr.predict(X_test_scaled)\n",
    "\n",
    "    ensemble_preds = (test_preds * 0.1 + xgb_test_preds* 0.3 + line_test_preds *0.6)\n",
    "\n",
    "# Metrics\n",
    "    #train_mae = mean_absolute_error(y_train_fold, train_preds)\n",
    "    test_mae = mean_absolute_error(y_test_fold, ensemble_preds)\n",
    "    test_line_mae = mean_absolute_error(y_test_fold, line_test_preds)\n",
    "    test_ridge_mae = mean_absolute_error(y_test_fold, test_preds)\n",
    "    test_xgb_mae = mean_absolute_error(y_test_fold, xgb_test_preds)\n",
    "    fold_results.append({\n",
    "        'fold': fold,\n",
    "        #'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'test_line_mae': test_line_mae,\n",
    "        'test_ridge_mae': test_ridge_mae,\n",
    "        'test_xgb_mae': test_xgb_mae})\n",
    "\n",
    "print(f\" Test MAE = {test_mae:.4f}\")\n",
    "print(f\" Test Line MAE = {test_line_mae:.4f}\")\n",
    "print(f\" Test Ridge MAE = {test_ridge_mae:.4f}\")\n",
    "print(f\" Test XGB MAE = {test_xgb_mae:.4f}\")\n",
    "# Summary\n",
    "results_df = pd.DataFrame(fold_results)\n",
    "print(\"\\nGroupKFold results by fold:\")\n",
    "print(results_df)\n",
    "print(\"\\nAverage across 5 folds:\")\n",
    "print(results_df.mean(numeric_only=True))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
